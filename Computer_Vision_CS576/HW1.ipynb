{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modifying.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfG_todxMT6c"
      },
      "source": [
        "CS576 Assignment #1: Image Classification using Bag of Visual Words (BoVW) \n",
        "====\n",
        "Primary TA : Jaehoon Yoo\n",
        "\n",
        "TA's E-mail : wogns98@kaist.ac.kr, whieya@kaist.ac.kr\n",
        "\n",
        "QnA Channel: https://join.slack.com/t/kaistcs576/shared_invite/zt-o3gqak0y-yj3NCb_SQFxVkqO0U6PWYw\n",
        "## Instruction\n",
        "- In this assignment, we will classify the images into five categories (aeroplane, backgrounds, car, horse, motorcycle, person) using Bag of Visual Word (BoVW) and Support Vector Machine (SVM).\n",
        " \n",
        "- We will extract the SIFT descriptors from the images and construct a codebook. After that, we will encode the images to histogram features using codebook, and train the classifier using those features.\n",
        "\n",
        "- As you follow the given steps, fill in the section marked ***Problem*** with the appropriate code. There are **7 problems** in total.\n",
        "    - For **Problem 1 ~ Problem 4**, you will get full credits (10pt each) if you implement correctly.  \n",
        "    - For **Problem 5 ~ Problem 7**, you **have to write a discussion about the results** as well as implementing the codes. Each problem takes 5pt for the correct implementation and 5 pt for proper discussion. In other words, you will get only 5pt without proper discussion even if you correctly implement the codes. To get full credit for discussion, please follow **Discussion Guidelines**.\n",
        "\n",
        "## Discussion Guidelines\n",
        "- You should write a discussion about **Problem 5 ~ Problem 7** on the **Discussion and Analysis** section. \n",
        "- Simply reporting the scores (e.g. classification accuracy) is not considered as a discussion.\n",
        "- For each problem's discussion, you should explain and compare how each method improves the results. \n",
        "\n",
        "## Submission guidelines\n",
        "- Your code and report will be all in Colab. Copy this example to your google drive and edit it to complete your assignment. \n",
        "- <font color=\"red\"> You will get the full credit **only if** you complete the code **and** write a discussion of the results in the discussion section at the bottom of this page. </font>\n",
        "- We should be able to reproduce your results using your code. Please double-check if your code runs without error and reproduces your results. Submissions failed to run or reproduce the results will get a substantial penalty. \n",
        "- <font color=\"red\"> **DO NOT modify any of the skeleton codes when you submit.** Please write your codes only in the designated area. </font>\n",
        "- As a proof that you've ran this code by yourself, **make sure your notebook contains the output of each code block.**\n",
        "\n",
        "## Deliverables\n",
        "- Download your Colab notebook, and submit it in a format: [StudentID].ipynb.\n",
        "- Your assignment should be submitted through KLMS. All other submissions (e.g., via email) will not be considered as valid submissions. \n",
        "\n",
        "## Due date\n",
        "- **23:59:59 April 7th.**\n",
        "- Late submission is allowed until 23:59:59 April 9th.\n",
        "- Late submission will be applied 20% penalty.\n",
        "\n",
        "\n",
        "\n",
        "## Questions\n",
        "- Please use the SLACK channel (https://join.slack.com/t/kaistcs576/shared_invite/zt-o3gqak0y-yj3NCb_SQFxVkqO0U6PWYw) as a main communication channel. \n",
        "When you post questions, please make it public so that all students can share the information. Please use the prefix \"[Assignment 1]\" in the subject for all questions regarding this assignment (e.g., [Assignment 1] Regarding the grading policy).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RysBzJQFeIkg"
      },
      "source": [
        "## Step 0: Set the enviroments\n",
        "For this assignment, you need the special library for extracting features & training classifier (cyvlfeat & sklearn).\n",
        "This step takes about 5~15 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26drrtRufRbK"
      },
      "source": [
        "###  0-1: Download cyvlfeat library & conda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEjDierhsAZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd4251b-4920-42ac-f145-85be66100d08"
      },
      "source": [
        "# install conda on colab\n",
        "!wget -c https://repo.continuum.io/archive/Anaconda3-5.3.1-Linux-x86_64.sh\n",
        "!chmod +x Anaconda3-5.3.1-Linux-x86_64.sh\n",
        "!bash ./Anaconda3-5.3.1-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# install cyvlfeat\n",
        "# Reference : https://anaconda.org/menpo/cyvlfeat\n",
        "# Update URL (2021/03/22)\n",
        "!conda install -c menpo cyvlfeat python==3.7 -y\n",
        "!conda install cython numpy scipy -y\n",
        "\n",
        "import sys\n",
        "sys.path.append('/cyvlfeat')\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "\n",
        "!git clone https://github.com/menpo/cyvlfeat.git /cyvlfeat\n",
        "!cd /cyvlfeat && CFLAGS=\"-I$CONDA_PREFIX/include\" LDFLAGS=\"-L$CONDA_PREFIX/lib\" pip install -e ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-04 07:26:23--  https://repo.continuum.io/archive/Anaconda3-5.3.1-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c84f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/archive/Anaconda3-5.3.1-Linux-x86_64.sh [following]\n",
            "--2021-04-04 07:26:23--  https://repo.anaconda.com/archive/Anaconda3-5.3.1-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 667976437 (637M) [application/x-sh]\n",
            "Saving to: ‘Anaconda3-5.3.1-Linux-x86_64.sh’\n",
            "\n",
            "Anaconda3-5.3.1-Lin 100%[===================>] 637.03M   225MB/s    in 2.8s    \n",
            "\n",
            "2021-04-04 07:26:26 (225 MB/s) - ‘Anaconda3-5.3.1-Linux-x86_64.sh’ saved [667976437/667976437]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "reinstalling: python-3.7.0-hc3d631a_0 ...\n",
            "Python 3.7.0\n",
            "reinstalling: blas-1.0-mkl ...\n",
            "reinstalling: ca-certificates-2018.03.07-0 ...\n",
            "reinstalling: conda-env-2.6.0-1 ...\n",
            "reinstalling: intel-openmp-2019.0-118 ...\n",
            "reinstalling: libgcc-ng-8.2.0-hdf63c60_1 ...\n",
            "reinstalling: libgfortran-ng-7.3.0-hdf63c60_0 ...\n",
            "reinstalling: libstdcxx-ng-8.2.0-hdf63c60_1 ...\n",
            "reinstalling: bzip2-1.0.6-h14c3975_5 ...\n",
            "reinstalling: expat-2.2.6-he6710b0_0 ...\n",
            "reinstalling: fribidi-1.0.5-h7b6447c_0 ...\n",
            "reinstalling: gmp-6.1.2-h6c8ec71_1 ...\n",
            "reinstalling: graphite2-1.3.12-h23475e2_2 ...\n",
            "reinstalling: icu-58.2-h9c2bf20_1 ...\n",
            "reinstalling: jbig-2.1-hdba287a_0 ...\n",
            "reinstalling: jpeg-9b-h024ee3a_2 ...\n",
            "reinstalling: libffi-3.2.1-hd88cf55_4 ...\n",
            "reinstalling: libsodium-1.0.16-h1bed415_0 ...\n",
            "reinstalling: libtool-2.4.6-h544aabb_3 ...\n",
            "reinstalling: libuuid-1.0.3-h1bed415_2 ...\n",
            "reinstalling: libxcb-1.13-h1bed415_1 ...\n",
            "reinstalling: lzo-2.10-h49e0be7_2 ...\n",
            "reinstalling: mkl-2019.0-118 ...\n",
            "reinstalling: ncurses-6.1-hf484d3e_0 ...\n",
            "reinstalling: openssl-1.0.2p-h14c3975_0 ...\n",
            "reinstalling: patchelf-0.9-hf484d3e_2 ...\n",
            "reinstalling: pcre-8.42-h439df22_0 ...\n",
            "reinstalling: pixman-0.34.0-hceecf20_3 ...\n",
            "reinstalling: snappy-1.1.7-hbae5bb6_3 ...\n",
            "reinstalling: xz-5.2.4-h14c3975_4 ...\n",
            "reinstalling: yaml-0.1.7-had09818_2 ...\n",
            "reinstalling: zlib-1.2.11-ha838bed_2 ...\n",
            "reinstalling: blosc-1.14.4-hdbcaa40_0 ...\n",
            "reinstalling: glib-2.56.2-hd408876_0 ...\n",
            "reinstalling: hdf5-1.10.2-hba1933b_1 ...\n",
            "reinstalling: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "reinstalling: libpng-1.6.34-hb9fc6fc_0 ...\n",
            "reinstalling: libssh2-1.8.0-h9cfc8f7_4 ...\n",
            "reinstalling: libtiff-4.0.9-he85c1e1_2 ...\n",
            "reinstalling: libxml2-2.9.8-h26e45fe_1 ...\n",
            "reinstalling: mpfr-4.0.1-hdf1c602_3 ...\n",
            "reinstalling: pandoc-1.19.2.1-hea2e7c5_1 ...\n",
            "reinstalling: readline-7.0-h7b6447c_5 ...\n",
            "reinstalling: tk-8.6.8-hbc83047_0 ...\n",
            "reinstalling: zeromq-4.2.5-hf484d3e_1 ...\n",
            "reinstalling: dbus-1.13.2-h714fa37_1 ...\n",
            "reinstalling: freetype-2.9.1-h8a8886c_1 ...\n",
            "reinstalling: gstreamer-1.14.0-hb453b48_1 ...\n",
            "reinstalling: libcurl-7.61.0-h1ad7b7a_0 ...\n",
            "reinstalling: libxslt-1.1.32-h1312cb7_0 ...\n",
            "reinstalling: mpc-1.1.0-h10f8cd9_1 ...\n",
            "reinstalling: sqlite-3.24.0-h84994c4_0 ...\n",
            "reinstalling: unixodbc-2.3.7-h14c3975_0 ...\n",
            "reinstalling: curl-7.61.0-h84994c4_0 ...\n",
            "reinstalling: fontconfig-2.13.0-h9420a91_0 ...\n",
            "reinstalling: gst-plugins-base-1.14.0-hbbd80ab_1 ...\n",
            "reinstalling: alabaster-0.7.11-py37_0 ...\n",
            "reinstalling: appdirs-1.4.3-py37h28b3542_0 ...\n",
            "reinstalling: asn1crypto-0.24.0-py37_0 ...\n",
            "reinstalling: atomicwrites-1.2.1-py37_0 ...\n",
            "reinstalling: attrs-18.2.0-py37h28b3542_0 ...\n",
            "reinstalling: backcall-0.1.0-py37_0 ...\n",
            "reinstalling: backports-1.0-py37_1 ...\n",
            "reinstalling: beautifulsoup4-4.6.3-py37_0 ...\n",
            "reinstalling: bitarray-0.8.3-py37h14c3975_0 ...\n",
            "reinstalling: boto-2.49.0-py37_0 ...\n",
            "reinstalling: cairo-1.14.12-h8948797_3 ...\n",
            "reinstalling: certifi-2018.8.24-py37_1 ...\n",
            "reinstalling: chardet-3.0.4-py37_1 ...\n",
            "reinstalling: click-6.7-py37_0 ...\n",
            "reinstalling: cloudpickle-0.5.5-py37_0 ...\n",
            "reinstalling: colorama-0.3.9-py37_0 ...\n",
            "reinstalling: constantly-15.1.0-py37h28b3542_0 ...\n",
            "reinstalling: contextlib2-0.5.5-py37_0 ...\n",
            "reinstalling: dask-core-0.19.1-py37_0 ...\n",
            "reinstalling: decorator-4.3.0-py37_0 ...\n",
            "reinstalling: defusedxml-0.5.0-py37_1 ...\n",
            "reinstalling: docutils-0.14-py37_0 ...\n",
            "reinstalling: entrypoints-0.2.3-py37_2 ...\n",
            "reinstalling: et_xmlfile-1.0.1-py37_0 ...\n",
            "reinstalling: fastcache-1.0.2-py37h14c3975_2 ...\n",
            "reinstalling: filelock-3.0.8-py37_0 ...\n",
            "reinstalling: glob2-0.6-py37_0 ...\n",
            "reinstalling: gmpy2-2.0.8-py37h10f8cd9_2 ...\n",
            "reinstalling: greenlet-0.4.15-py37h7b6447c_0 ...\n",
            "reinstalling: heapdict-1.0.0-py37_2 ...\n",
            "reinstalling: idna-2.7-py37_0 ...\n",
            "reinstalling: imagesize-1.1.0-py37_0 ...\n",
            "reinstalling: incremental-17.5.0-py37_0 ...\n",
            "reinstalling: ipython_genutils-0.2.0-py37_0 ...\n",
            "reinstalling: itsdangerous-0.24-py37_1 ...\n",
            "reinstalling: jdcal-1.4-py37_0 ...\n",
            "reinstalling: jeepney-0.3.1-py37_0 ...\n",
            "reinstalling: kiwisolver-1.0.1-py37hf484d3e_0 ...\n",
            "reinstalling: lazy-object-proxy-1.3.1-py37h14c3975_2 ...\n",
            "reinstalling: llvmlite-0.24.0-py37hdbcaa40_0 ...\n",
            "reinstalling: locket-0.2.0-py37_1 ...\n",
            "reinstalling: lxml-4.2.5-py37hefd8a0e_0 ...\n",
            "reinstalling: markupsafe-1.0-py37h14c3975_1 ...\n",
            "reinstalling: mccabe-0.6.1-py37_1 ...\n",
            "reinstalling: mistune-0.8.3-py37h14c3975_1 ...\n",
            "reinstalling: mkl-service-1.1.2-py37h90e4bf4_5 ...\n",
            "reinstalling: mpmath-1.0.0-py37_2 ...\n",
            "reinstalling: msgpack-python-0.5.6-py37h6bb024c_1 ...\n",
            "reinstalling: numpy-base-1.15.1-py37h81de0dd_0 ...\n",
            "reinstalling: olefile-0.46-py37_0 ...\n",
            "reinstalling: pandocfilters-1.4.2-py37_1 ...\n",
            "reinstalling: parso-0.3.1-py37_0 ...\n",
            "reinstalling: path.py-11.1.0-py37_0 ...\n",
            "reinstalling: pep8-1.7.1-py37_0 ...\n",
            "reinstalling: pickleshare-0.7.4-py37_0 ...\n",
            "reinstalling: pkginfo-1.4.2-py37_1 ...\n",
            "reinstalling: pluggy-0.7.1-py37h28b3542_0 ...\n",
            "reinstalling: ply-3.11-py37_0 ...\n",
            "reinstalling: psutil-5.4.7-py37h14c3975_0 ...\n",
            "reinstalling: ptyprocess-0.6.0-py37_0 ...\n",
            "reinstalling: py-1.6.0-py37_0 ...\n",
            "reinstalling: pyasn1-0.4.4-py37h28b3542_0 ...\n",
            "reinstalling: pycodestyle-2.4.0-py37_0 ...\n",
            "reinstalling: pycosat-0.6.3-py37h14c3975_0 ...\n",
            "reinstalling: pycparser-2.18-py37_1 ...\n",
            "reinstalling: pycrypto-2.6.1-py37h14c3975_9 ...\n",
            "reinstalling: pycurl-7.43.0.2-py37hb7f436b_0 ...\n",
            "reinstalling: pyflakes-2.0.0-py37_0 ...\n",
            "reinstalling: pyodbc-4.0.24-py37he6710b0_0 ...\n",
            "reinstalling: pyparsing-2.2.0-py37_1 ...\n",
            "reinstalling: pysocks-1.6.8-py37_0 ...\n",
            "reinstalling: pytz-2018.5-py37_0 ...\n",
            "reinstalling: pyyaml-3.13-py37h14c3975_0 ...\n",
            "reinstalling: pyzmq-17.1.2-py37h14c3975_0 ...\n",
            "reinstalling: qt-5.9.6-h8703b6f_2 ...\n",
            "reinstalling: qtpy-1.5.0-py37_0 ...\n",
            "reinstalling: rope-0.11.0-py37_0 ...\n",
            "reinstalling: ruamel_yaml-0.15.46-py37h14c3975_0 ...\n",
            "reinstalling: send2trash-1.5.0-py37_0 ...\n",
            "reinstalling: simplegeneric-0.8.1-py37_2 ...\n",
            "reinstalling: sip-4.19.8-py37hf484d3e_0 ...\n",
            "reinstalling: six-1.11.0-py37_1 ...\n",
            "reinstalling: snowballstemmer-1.2.1-py37_0 ...\n",
            "reinstalling: sortedcontainers-2.0.5-py37_0 ...\n",
            "reinstalling: sphinxcontrib-1.0-py37_1 ...\n",
            "reinstalling: sqlalchemy-1.2.11-py37h7b6447c_0 ...\n",
            "reinstalling: tblib-1.3.2-py37_0 ...\n",
            "reinstalling: testpath-0.3.1-py37_0 ...\n",
            "reinstalling: toolz-0.9.0-py37_0 ...\n",
            "reinstalling: tornado-5.1-py37h14c3975_0 ...\n",
            "reinstalling: tqdm-4.26.0-py37h28b3542_0 ...\n",
            "reinstalling: unicodecsv-0.14.1-py37_0 ...\n",
            "reinstalling: wcwidth-0.1.7-py37_0 ...\n",
            "reinstalling: webencodings-0.5.1-py37_1 ...\n",
            "reinstalling: werkzeug-0.14.1-py37_0 ...\n",
            "reinstalling: wrapt-1.10.11-py37h14c3975_2 ...\n",
            "reinstalling: xlrd-1.1.0-py37_1 ...\n",
            "reinstalling: xlsxwriter-1.1.0-py37_0 ...\n",
            "reinstalling: xlwt-1.3.0-py37_0 ...\n",
            "reinstalling: zope-1.0-py37_1 ...\n",
            "reinstalling: astroid-2.0.4-py37_0 ...\n",
            "reinstalling: automat-0.7.0-py37_0 ...\n",
            "reinstalling: babel-2.6.0-py37_0 ...\n",
            "reinstalling: backports.shutil_get_terminal_size-1.0.0-py37_2 ...\n",
            "reinstalling: cffi-1.11.5-py37he75722e_1 ...\n",
            "reinstalling: cycler-0.10.0-py37_0 ...\n",
            "reinstalling: cytoolz-0.9.0.1-py37h14c3975_1 ...\n",
            "reinstalling: harfbuzz-1.8.8-hffaf4a1_0 ...\n",
            "reinstalling: html5lib-1.0.1-py37_0 ...\n",
            "reinstalling: hyperlink-18.0.0-py37_0 ...\n",
            "reinstalling: jedi-0.12.1-py37_0 ...\n",
            "reinstalling: more-itertools-4.3.0-py37_0 ...\n",
            "reinstalling: multipledispatch-0.6.0-py37_0 ...\n",
            "reinstalling: networkx-2.1-py37_0 ...\n",
            "reinstalling: nltk-3.3.0-py37_0 ...\n",
            "reinstalling: openpyxl-2.5.6-py37_0 ...\n",
            "reinstalling: packaging-17.1-py37_0 ...\n",
            "reinstalling: partd-0.3.8-py37_0 ...\n",
            "reinstalling: pathlib2-2.3.2-py37_0 ...\n",
            "reinstalling: pexpect-4.6.0-py37_0 ...\n",
            "reinstalling: pillow-5.2.0-py37heded4f4_0 ...\n",
            "reinstalling: pyasn1-modules-0.2.2-py37_0 ...\n",
            "reinstalling: pyqt-5.9.2-py37h05f1152_2 ...\n",
            "reinstalling: python-dateutil-2.7.3-py37_0 ...\n",
            "reinstalling: qtawesome-0.4.4-py37_0 ...\n",
            "reinstalling: setuptools-40.2.0-py37_0 ...\n",
            "reinstalling: singledispatch-3.4.0.3-py37_0 ...\n",
            "reinstalling: sortedcollections-1.0.1-py37_0 ...\n",
            "reinstalling: sphinxcontrib-websupport-1.1.0-py37_1 ...\n",
            "reinstalling: sympy-1.2-py37_0 ...\n",
            "reinstalling: terminado-0.8.1-py37_1 ...\n",
            "reinstalling: traitlets-4.3.2-py37_0 ...\n",
            "reinstalling: zict-0.1.3-py37_0 ...\n",
            "reinstalling: zope.interface-4.5.0-py37h14c3975_0 ...\n",
            "reinstalling: bleach-2.1.4-py37_0 ...\n",
            "reinstalling: clyent-1.2.2-py37_1 ...\n",
            "reinstalling: cryptography-2.3.1-py37hc365091_0 ...\n",
            "reinstalling: cython-0.28.5-py37hf484d3e_0 ...\n",
            "reinstalling: distributed-1.23.1-py37_0 ...\n",
            "reinstalling: get_terminal_size-1.0.0-haa9412d_0 ...\n",
            "reinstalling: gevent-1.3.6-py37h7b6447c_0 ...\n",
            "reinstalling: isort-4.3.4-py37_0 ...\n",
            "reinstalling: jinja2-2.10-py37_0 ...\n",
            "reinstalling: jsonschema-2.6.0-py37_0 ...\n",
            "reinstalling: jupyter_core-4.4.0-py37_0 ...\n",
            "reinstalling: navigator-updater-0.2.1-py37_0 ...\n",
            "reinstalling: nose-1.3.7-py37_2 ...\n",
            "reinstalling: pango-1.42.4-h049681c_0 ...\n",
            "reinstalling: pygments-2.2.0-py37_0 ...\n",
            "reinstalling: pytest-3.8.0-py37_0 ...\n",
            "reinstalling: wheel-0.31.1-py37_0 ...\n",
            "reinstalling: flask-1.0.2-py37_1 ...\n",
            "reinstalling: jupyter_client-5.2.3-py37_0 ...\n",
            "reinstalling: nbformat-4.4.0-py37_0 ...\n",
            "reinstalling: pip-10.0.1-py37_0 ...\n",
            "reinstalling: prompt_toolkit-1.0.15-py37_0 ...\n",
            "reinstalling: pylint-2.1.1-py37_0 ...\n",
            "reinstalling: pyopenssl-18.0.0-py37_0 ...\n",
            "reinstalling: pytest-openfiles-0.3.0-py37_0 ...\n",
            "reinstalling: pytest-remotedata-0.3.0-py37_0 ...\n",
            "reinstalling: secretstorage-3.1.0-py37_0 ...\n",
            "reinstalling: flask-cors-3.0.6-py37_0 ...\n",
            "reinstalling: ipython-6.5.0-py37_0 ...\n",
            "reinstalling: keyring-13.2.1-py37_0 ...\n",
            "reinstalling: nbconvert-5.4.0-py37_1 ...\n",
            "reinstalling: service_identity-17.0.0-py37h28b3542_0 ...\n",
            "reinstalling: urllib3-1.23-py37_0 ...\n",
            "reinstalling: ipykernel-4.9.0-py37_1 ...\n",
            "reinstalling: requests-2.19.1-py37_0 ...\n",
            "reinstalling: twisted-18.7.0-py37h14c3975_1 ...\n",
            "reinstalling: anaconda-client-1.7.2-py37_0 ...\n",
            "reinstalling: jupyter_console-5.2.0-py37_1 ...\n",
            "reinstalling: prometheus_client-0.3.1-py37h28b3542_0 ...\n",
            "reinstalling: qtconsole-4.4.1-py37_0 ...\n",
            "reinstalling: sphinx-1.7.9-py37_0 ...\n",
            "reinstalling: spyder-kernels-0.2.6-py37_0 ...\n",
            "reinstalling: anaconda-navigator-1.9.2-py37_0 ...\n",
            "reinstalling: anaconda-project-0.8.2-py37_0 ...\n",
            "reinstalling: notebook-5.6.0-py37_0 ...\n",
            "reinstalling: numpydoc-0.8.0-py37_0 ...\n",
            "reinstalling: jupyterlab_launcher-0.13.1-py37_0 ...\n",
            "reinstalling: spyder-3.3.1-py37_1 ...\n",
            "reinstalling: widgetsnbextension-3.4.1-py37_0 ...\n",
            "reinstalling: ipywidgets-7.4.1-py37_0 ...\n",
            "reinstalling: jupyterlab-0.34.9-py37_0 ...\n",
            "reinstalling: _ipyw_jlab_nb_ext_conf-0.1.0-py37_0 ...\n",
            "reinstalling: jupyter-1.0.0-py37_7 ...\n",
            "reinstalling: bokeh-0.13.0-py37_0 ...\n",
            "reinstalling: bottleneck-1.2.1-py37h035aef0_1 ...\n",
            "reinstalling: conda-4.5.11-py37_0 ...\n",
            "reinstalling: conda-build-3.15.1-py37_0 ...\n",
            "reinstalling: datashape-0.5.4-py37_1 ...\n",
            "reinstalling: h5py-2.8.0-py37h989c5e5_3 ...\n",
            "reinstalling: imageio-2.4.1-py37_0 ...\n",
            "reinstalling: matplotlib-2.2.3-py37hb69df0a_0 ...\n",
            "reinstalling: mkl_fft-1.0.4-py37h4414c95_1 ...\n",
            "reinstalling: mkl_random-1.0.1-py37h4414c95_1 ...\n",
            "reinstalling: numpy-1.15.1-py37h1d66e8a_0 ...\n",
            "reinstalling: numba-0.39.0-py37h04863e7_0 ...\n",
            "reinstalling: numexpr-2.6.8-py37hd89afb7_0 ...\n",
            "reinstalling: pandas-0.23.4-py37h04863e7_0 ...\n",
            "reinstalling: pytest-arraydiff-0.2-py37h39e3cac_0 ...\n",
            "reinstalling: pytest-doctestplus-0.1.3-py37_0 ...\n",
            "reinstalling: pywavelets-1.0.0-py37hdd07704_0 ...\n",
            "reinstalling: scipy-1.1.0-py37hfa4b5c9_1 ...\n",
            "reinstalling: bkcharts-0.2-py37_0 ...\n",
            "reinstalling: dask-0.19.1-py37_0 ...\n",
            "reinstalling: patsy-0.5.0-py37_0 ...\n",
            "reinstalling: pytables-3.4.4-py37ha205bf6_0 ...\n",
            "reinstalling: pytest-astropy-0.4.0-py37_0 ...\n",
            "reinstalling: scikit-image-0.14.0-py37hf484d3e_1 ...\n",
            "reinstalling: scikit-learn-0.19.2-py37h4989274_0 ...\n",
            "reinstalling: astropy-3.0.4-py37h14c3975_0 ...\n",
            "reinstalling: odo-0.5.1-py37_0 ...\n",
            "reinstalling: statsmodels-0.9.0-py37h035aef0_0 ...\n",
            "reinstalling: blaze-0.11.3-py37_0 ...\n",
            "reinstalling: seaborn-0.9.0-py37_0 ...\n",
            "reinstalling: anaconda-5.3.1-py37_0 ...\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Anaconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Anaconda3: /usr/local\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.5.11\n",
            "  latest version: 4.9.2\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c defaults conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - cyvlfeat\n",
            "    - python==3.7\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    sqlite-3.33.0              |       h62c20be_0         2.0 MB\n",
            "    pip-21.0.1                 |   py37h06a4308_0         2.0 MB\n",
            "    tk-8.6.10                  |       hbc83047_0         3.2 MB\n",
            "    mkl_random-1.1.1           |   py37h0573a6f_0         375 KB\n",
            "    numpy-base-1.19.2          |   py37hfa32c7d_0         5.2 MB\n",
            "    libffi-3.2.1               |    hf484d3e_1007          52 KB\n",
            "    mkl-service-2.3.0          |   py37he8ac12f_0          56 KB\n",
            "    six-1.15.0                 |     pyhd3eb1b0_0          13 KB\n",
            "    cyvlfeat-0.5.1             |   py37h975b26e_0         2.0 MB  menpo\n",
            "    numpy-1.19.2               |   py37h54aff64_0          21 KB\n",
            "    _libgcc_mutex-0.1          |             main           3 KB\n",
            "    vlfeat-0.9.20              |                1         199 KB  menpo\n",
            "    setuptools-52.0.0          |   py37h06a4308_0         921 KB\n",
            "    libedit-3.1.20210216       |       h27cfd23_1         190 KB\n",
            "    ca-certificates-2021.1.19  |       h06a4308_1         125 KB\n",
            "    mkl-2020.2                 |              256       213.9 MB\n",
            "    openssl-1.0.2u             |       h7b6447c_0         3.1 MB\n",
            "    xz-5.2.5                   |       h7b6447c_0         438 KB\n",
            "    zlib-1.2.11                |       h7b6447c_3         120 KB\n",
            "    certifi-2020.12.5          |   py37h06a4308_0         143 KB\n",
            "    mkl_fft-1.3.0              |   py37h54f3939_0         185 KB\n",
            "    wheel-0.36.2               |     pyhd3eb1b0_0          31 KB\n",
            "    libgcc-ng-9.1.0            |       hdf63c60_0         8.1 MB\n",
            "    python-3.7.0               |       h6e4f718_3        30.6 MB\n",
            "    ncurses-6.2                |       he6710b0_1         1.1 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       274.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:   0.1-main                     \n",
            "    cyvlfeat:        0.5.1-py37h975b26e_0    menpo\n",
            "    vlfeat:          0.9.20-1                menpo\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates: 2018.03.07-0                  --> 2021.1.19-h06a4308_1    \n",
            "    certifi:         2018.8.24-py37_1              --> 2020.12.5-py37h06a4308_0\n",
            "    libedit:         3.1.20170329-h6b74fdf_2       --> 3.1.20210216-h27cfd23_1 \n",
            "    libffi:          3.2.1-hd88cf55_4              --> 3.2.1-hf484d3e_1007     \n",
            "    libgcc-ng:       8.2.0-hdf63c60_1              --> 9.1.0-hdf63c60_0        \n",
            "    mkl:             2019.0-118                    --> 2020.2-256              \n",
            "    mkl-service:     1.1.2-py37h90e4bf4_5          --> 2.3.0-py37he8ac12f_0    \n",
            "    mkl_fft:         1.0.4-py37h4414c95_1          --> 1.3.0-py37h54f3939_0    \n",
            "    mkl_random:      1.0.1-py37h4414c95_1          --> 1.1.1-py37h0573a6f_0    \n",
            "    ncurses:         6.1-hf484d3e_0                --> 6.2-he6710b0_1          \n",
            "    numpy:           1.15.1-py37h1d66e8a_0         --> 1.19.2-py37h54aff64_0   \n",
            "    numpy-base:      1.15.1-py37h81de0dd_0         --> 1.19.2-py37hfa32c7d_0   \n",
            "    openssl:         1.0.2p-h14c3975_0             --> 1.0.2u-h7b6447c_0       \n",
            "    pip:             10.0.1-py37_0                 --> 21.0.1-py37h06a4308_0   \n",
            "    python:          3.7.0-hc3d631a_0              --> 3.7.0-h6e4f718_3        \n",
            "    setuptools:      40.2.0-py37_0                 --> 52.0.0-py37h06a4308_0   \n",
            "    six:             1.11.0-py37_1                 --> 1.15.0-pyhd3eb1b0_0     \n",
            "    sqlite:          3.24.0-h84994c4_0             --> 3.33.0-h62c20be_0       \n",
            "    tk:              8.6.8-hbc83047_0              --> 8.6.10-hbc83047_0       \n",
            "    wheel:           0.31.1-py37_0                 --> 0.36.2-pyhd3eb1b0_0     \n",
            "    xz:              5.2.4-h14c3975_4              --> 5.2.5-h7b6447c_0        \n",
            "    zlib:            1.2.11-ha838bed_2             --> 1.2.11-h7b6447c_3       \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "sqlite-3.33.0        | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  3.21it/s]              \n",
            "pip-21.0.1           | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.79it/s]              \n",
            "tk-8.6.10            | 3.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.66it/s]               \n",
            "mkl_random-1.1.1     | 375 KB    | : 100% 1.0/1 [00:00<00:00, 11.20it/s]\n",
            "numpy-base-1.19.2    | 5.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.12s/it]               \n",
            "libffi-3.2.1         | 52 KB     | : 100% 1.0/1 [00:00<00:00, 31.47it/s]\n",
            "mkl-service-2.3.0    | 56 KB     | : 100% 1.0/1 [00:00<00:00, 32.78it/s]\n",
            "six-1.15.0           | 13 KB     | : 100% 1.0/1 [00:00<00:00, 36.90it/s]\n",
            "cyvlfeat-0.5.1       | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.42it/s]               \n",
            "numpy-1.19.2         | 21 KB     | : 100% 1.0/1 [00:00<00:00, 28.68it/s]\n",
            "_libgcc_mutex-0.1    | 3 KB      | : 100% 1.0/1 [00:00<00:00, 31.37it/s]\n",
            "vlfeat-0.9.20        | 199 KB    | : 100% 1.0/1 [00:00<00:00,  2.40it/s]               \n",
            "setuptools-52.0.0    | 921 KB    | : 100% 1.0/1 [00:00<00:00,  3.50it/s]               \n",
            "libedit-3.1.20210216 | 190 KB    | : 100% 1.0/1 [00:00<00:00, 11.67it/s]\n",
            "ca-certificates-2021 | 125 KB    | : 100% 1.0/1 [00:00<00:00, 25.29it/s]\n",
            "mkl-2020.2           | 213.9 MB  | : 100% 1.0/1 [00:36<00:00, 36.04s/it]                \n",
            "openssl-1.0.2u       | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.82it/s]               \n",
            "xz-5.2.5             | 438 KB    | : 100% 1.0/1 [00:00<00:00,  8.91it/s]\n",
            "zlib-1.2.11          | 120 KB    | : 100% 1.0/1 [00:00<00:00, 20.03it/s]\n",
            "certifi-2020.12.5    | 143 KB    | : 100% 1.0/1 [00:00<00:00, 24.05it/s]\n",
            "mkl_fft-1.3.0        | 185 KB    | : 100% 1.0/1 [00:00<00:00, 17.84it/s]\n",
            "wheel-0.36.2         | 31 KB     | : 100% 1.0/1 [00:00<00:00, 29.92it/s]\n",
            "libgcc-ng-9.1.0      | 8.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.13s/it]               \n",
            "python-3.7.0         | 30.6 MB   | : 100% 1.0/1 [00:04<00:00,  4.20s/it]               \n",
            "ncurses-6.2          | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.38it/s]               \n",
            "Preparing transaction: | \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - cython\n",
            "    - numpy\n",
            "    - scipy\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    cython-0.29.22             |   py37h2531618_0         2.1 MB\n",
            "    scipy-1.6.2                |   py37h91f5cce_0        19.9 MB\n",
            "    conda-4.9.2                |   py37h06a4308_0         3.1 MB\n",
            "    conda-package-handling-1.7.2|   py37h03888b9_0         977 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        26.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    conda-package-handling: 1.7.2-py37h03888b9_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    conda:                  4.5.11-py37_0         --> 4.9.2-py37h06a4308_0  \n",
            "    cython:                 0.28.5-py37hf484d3e_0 --> 0.29.22-py37h2531618_0\n",
            "    scipy:                  1.1.0-py37hfa4b5c9_1  --> 1.6.2-py37h91f5cce_0  \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "cython-0.29.22       | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.32it/s]               \n",
            "scipy-1.6.2          | 19.9 MB   | : 100% 1.0/1 [00:03<00:00,  3.40s/it]              \n",
            "conda-4.9.2          | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.55it/s]              \n",
            "conda-package-handli | 977 KB    | : 100% 1.0/1 [00:00<00:00,  6.10it/s]               \n",
            "Preparing transaction: | \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Cloning into '/cyvlfeat'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 1097 (delta 9), reused 23 (delta 7), pack-reused 1049\u001b[K\n",
            "Receiving objects: 100% (1097/1097), 1.38 MiB | 16.21 MiB/s, done.\n",
            "Resolving deltas: 100% (602/602), done.\n",
            "Obtaining file:///cyvlfeat\n",
            "Installing collected packages: cyvlfeat\n",
            "  Attempting uninstall: cyvlfeat\n",
            "    Found existing installation: cyvlfeat 0.5.1\n",
            "    Uninstalling cyvlfeat-0.5.1:\n",
            "      Successfully uninstalled cyvlfeat-0.5.1\n",
            "  Running setup.py develop for cyvlfeat\n",
            "Successfully installed cyvlfeat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3985gb9aOypG"
      },
      "source": [
        "###  0-2: Connect to your Google Drive.\n",
        "\n",
        "It is required for loading the data.\n",
        "\n",
        "Enter your authorization code to access your drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKffRxrvDSJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d9e4d23-4078-474f-daf1-2366d9aabc71"
      },
      "source": [
        "# mount drive https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bypm5tteROL"
      },
      "source": [
        "### 0-3: Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W88TOaCsxpfw"
      },
      "source": [
        "# Import libraries\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import cyvlfeat\n",
        "import time\n",
        "import scipy\n",
        "import multiprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xv7wrsXBO-w"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTq8GkOJBN4b"
      },
      "source": [
        "def euclidean_dist(x, y):\n",
        "    \"\"\"\n",
        "    :param x: [m, d]\n",
        "    :param y: [n, d]\n",
        "    :return:[m, n]\n",
        "    \"\"\"\n",
        "    m, n = x.shape[0], y.shape[0]    \n",
        "    eps = 1e-6 \n",
        "\n",
        "    xx = np.tile(np.power(x, 2).sum(axis=1), (n,1)) #[n, m]\n",
        "    xx = np.transpose(xx) # [m, n]\n",
        "    yy = np.tile(np.power(y, 2).sum(axis=1), (m,1)) #[m, n]\n",
        "    xy = np.matmul(x, np.transpose(y)) # [m, n]\n",
        "    dist = np.sqrt(xx + yy - 2*xy + eps)\n",
        "\n",
        "    return dist\n",
        "\n",
        "def read_img(image_path):\n",
        "    img = Image.open(image_path).convert('L')\n",
        "    img = img.resize((480, 480))\n",
        "    return np.float32(np.array(img)/255.)\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        data = f.read()\n",
        "    return data.split()\n",
        "    \n",
        "def dataset_setup(data_dir):\n",
        "    train_file_list = []\n",
        "    val_file_list = []\n",
        "\n",
        "    for class_name in ['aeroplane','background','car','horse','motorbike','person']:\n",
        "        train_txt_path = os.path.join(data_dir, class_name+'_train.txt')\n",
        "        train_file_list.append(np.array(read_txt(train_txt_path)))\n",
        "        val_txt_path = os.path.join(data_dir, class_name+'_val.txt')\n",
        "        val_file_list.append(np.array(read_txt(val_txt_path)))\n",
        "\n",
        "    train_file_list = np.unique(np.concatenate(train_file_list))\n",
        "    val_file_list = np.unique(np.concatenate(val_file_list))\n",
        "\n",
        "    f = open(os.path.join(data_dir, \"train.txt\"), 'w')\n",
        "    for i in range(train_file_list.shape[0]):\n",
        "        data = \"%s\\n\" % train_file_list[i]\n",
        "        f.write(data)\n",
        "    f.close()\n",
        "\n",
        "    f = open(os.path.join(data_dir, \"val.txt\"), 'w')\n",
        "    for i in range(val_file_list.shape[0]):\n",
        "        data = \"%s\\n\" % val_file_list[i]\n",
        "        f.write(data)\n",
        "    f.close()\n",
        "\n",
        "def load_train_data(data_dir):\n",
        "    dataset_setup(data_dir)\n",
        "    num_proc = 12 # num_process\n",
        "\n",
        "    txt_path = os.path.join(data_dir, 'train.txt')\n",
        "    file_list = read_txt(txt_path)\n",
        "    image_paths = [os.path.join(data_dir+'/images', file_name+'.jpg') for file_name in file_list]\n",
        "    with multiprocessing.Pool(num_proc) as pool:\n",
        "      imgs = pool.map(read_img, image_paths)\n",
        "      imgs = np.array(imgs)\n",
        "      idxs = np.array(file_list)\n",
        "\n",
        "    return imgs, idxs\n",
        "\n",
        "def load_val_data(data_dir):\n",
        "    dataset_setup(data_dir)\n",
        "    num_proc = 12 # num_process\n",
        "\n",
        "    txt_path = os.path.join(data_dir, 'val.txt')\n",
        "    file_list = read_txt(txt_path)\n",
        "    image_paths = [os.path.join(data_dir+'/images', file_name+'.jpg') for file_name in file_list]\n",
        "    with multiprocessing.Pool(num_proc) as pool:\n",
        "      imgs = pool.map(read_img, image_paths)\n",
        "      imgs = np.array(imgs)\n",
        "      idxs = np.array(file_list)\n",
        "    \n",
        "    return imgs, idxs\n",
        "\n",
        "def get_labels(idxs, target_idxs):\n",
        "    \"\"\"\n",
        "    Get the labels from file index(name).\n",
        "\n",
        "    :param idxs(numpy.array): file index(name). shape:[num_images, ]\n",
        "    :param target_idxs(numpy.array): target index(name). shape:[num_target,]\n",
        "    :return(numpy.array): Target label(Binary label consisting of True and False). shape:[num_images,]\n",
        "    \"\"\"\n",
        "    return np.isin(idxs, target_idxs)\n",
        "\n",
        "def load_train_idxs(data_dir):\n",
        "    txt_path = os.path.join(data_dir, 'train.txt')\n",
        "    train_idxs = np.array(read_txt(txt_path))\n",
        "    return train_idxs\n",
        "\n",
        "def load_val_idxs(data_dir):\n",
        "    txt_path = os.path.join(data_dir, 'val.txt')\n",
        "    val_idxs = np.array(read_txt(txt_path))\n",
        "    return val_idxs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c5F-N9wfzZW"
      },
      "source": [
        "## Step 1: Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYFEIkL24nJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d25d952-cc92-42fb-d375-231dd7ccacad"
      },
      "source": [
        "''' \n",
        "Set your data path for loading images & labels.\n",
        "Example) CS_DATA_DIR = '/gdrive/My Drive/data'\n",
        "'''\n",
        "\n",
        "%env CS_DATA_DIR=/gdrive/My Drive/data\n",
        "!mkdir -p \"$CS_DATA_DIR\"\n",
        "!cd \"$CS_DATA_DIR\" && wget http://www.di.ens.fr/willow/events/cvml2013/materials/practicals/category-level/practical-category-recognition-2013a-data-only.tar.gz && tar -zxf practical-category-recognition-2013a-data-only.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: CS_DATA_DIR=/gdrive/My Drive/data\n",
            "--2021-04-03 15:24:16--  http://www.di.ens.fr/willow/events/cvml2013/materials/practicals/category-level/practical-category-recognition-2013a-data-only.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/willow/events/cvml2013/materials/practicals/category-level/practical-category-recognition-2013a-data-only.tar.gz [following]\n",
            "--2021-04-03 15:24:16--  https://www.di.ens.fr/willow/events/cvml2013/materials/practicals/category-level/practical-category-recognition-2013a-data-only.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘practical-category-recognition-2013a-data-only.tar.gz’\n",
            "\n",
            "practical-category-     [ <=>                ] 964.15M  3.71MB/s    in 4m 19s  \n",
            "\n",
            "2021-04-03 15:28:36 (3.73 MB/s) - ‘practical-category-recognition-2013a-data-only.tar.gz’ saved [1010984641]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GW7H_2iPxzb"
      },
      "source": [
        "category = ['aeroplane', 'car', 'horse', 'motorbike', 'person'] # DON'T MODIFY THIS.\n",
        "data_dir = os.path.join(os.environ[\"CS_DATA_DIR\"], \"practical-category-recognition-2013a\", \"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX17mbhpXrNd"
      },
      "source": [
        "## Step 2: Bag of Visual Words (BoVW) Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QuLZmSxX2l5"
      },
      "source": [
        "### 2-1. (**Problem 1**): SIFT descriptor extraction & Save the descriptors (10pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EWqpgbOV6yE"
      },
      "source": [
        "def SIFT_extraction(imgs):\n",
        "    \"\"\"\n",
        "    Extract Local SIFT descriptors from images using cyvlfeat.sift.sift().\n",
        "    Refer to https://github.com/menpo/cyvlfeat\n",
        "    You should set the parameters of cyvlfeat.sift.sift() as bellow.\n",
        "    1.compute_descriptor = True  2.float_descriptors = True\n",
        "\n",
        "    :param train_imgs(numpy.array): Gray-scale images in Numpy array format. shape:[num_images, width_size, height_size]\n",
        "    :return(numpy.array): SIFT descriptors. shape:[num_images, ], ndarray with object(descripotrs)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    collect_descriptor = []\n",
        "    for i in range(imgs.shape[0]):\n",
        "      f, d = cyvlfeat.sift.sift(imgs[i],compute_descriptor = True, float_descriptors = True)\n",
        "      collect_descriptor.append(d)\n",
        "    return np.array(collect_descriptor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmVgUvbVYS2x"
      },
      "source": [
        "### 2-2. (**Problem 2**): Codebook(Bag of Visual Words) construction (10pt)\n",
        "In this step, you will construct the codebook using K-means clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLFB9eaw95zo"
      },
      "source": [
        "def get_codebook(des , k):\n",
        "  \"\"\"\n",
        "  Construct the codebook with visual codewords using k-means clustering.\n",
        "  In this step, you should use cyvlfeat.kmeans.kmeans().\n",
        "  Refer to https://github.com/menpo/cyvlfeat\n",
        "\n",
        "  :param des(numpy.array): Descriptors.  shape:[num_images, num_des_of_each_img, 128]\n",
        "  :param k(int): Number of visual words.\n",
        "  :return(numpy.array): Bag of visual words shape:[k, 128]\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  all_des = []\n",
        "  for i in range(des.shape[0]):\n",
        "    for j in range(des[i].shape[0]):\n",
        "      all_des.append(des[i][j]) \n",
        "  all_des = np.array(all_des) \n",
        "  centers = cyvlfeat.kmeans.kmeans(data=all_des, num_centers=k)\n",
        "  return centers  # ceneters is numpy array type "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH92-UOaYiM2"
      },
      "source": [
        "### 2-3. (**Problem 3**): Encode images to histogram feature based on codewords (10pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPQErulqCEKv"
      },
      "source": [
        "def extract_features(des, codebook):\n",
        "  \"\"\"\n",
        "  Construct the Bag-of-visual-Words histogram features for images using the codebook.\n",
        "  HINT: Refer to helper functions.\n",
        "\n",
        "  :param des(numpy.array): Descriptors.  shape:[num_images, num_des_of_each_img, 128]\n",
        "  :param codebook(numpy.array): Bag of visual words. shape:[k, 128]\n",
        "  :return(numpy.array): Bag of visual words shape:[num_images, k]\n",
        "\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  BOW = np.zeros(shape=(des.shape[0],codebook.shape[0]), dtype='float32')\n",
        "  for i in range(des.shape[0]):\n",
        "    distance_matrix = euclidean_dist(des[i], codebook)\n",
        "    # distance_matrix.shape = (num_des_of_each_img, k)\n",
        "    # num_des_of_each_img can be different dependent to image\n",
        "    min_indexs = np.argmin(distance_matrix,axis=1) # a list of element in range [0,..k-1]\n",
        "    for j in range(len(min_indexs)):\n",
        "      BOW[i][min_indexs[j]] += 1\n",
        "  return BOW     #BOW has type float32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJwCm_b3YwYe"
      },
      "source": [
        "## Step 3. (**Problem 4**): Train the classifiers (10pt)\n",
        "Train a classifier using the sklearn library (SVC) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gOjAvXXGJy"
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkFInH3bDJPV"
      },
      "source": [
        "def train_classifier(features, labels, svm_params):\n",
        "  \"\"\"\n",
        "  Train the SVM classifier using sklearn.svm.svc()\n",
        "  Refer to https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "  :param features(numpy.array): Historgram representation. shape:[num_images, dim_feature]\n",
        "  :param labels(numpy.array): Target label(binary). shape:[num_images,]\n",
        "  :return(sklearn.svm.SVC): Trained classifier\n",
        "  \"\"\"\n",
        "  # Your code here\n",
        "  clf = SVC(**svm_params)\n",
        "  clf.fit(features, labels)\n",
        "  return clf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNvZlykjWfyn"
      },
      "source": [
        "def Trainer(feat_params, svm_params):\n",
        "    \n",
        "    \"\"\"\n",
        "    Train the SVM classifier.\n",
        "\n",
        "    :param feat_params(dict): parameters for feature extraction.\n",
        "        ['extractor'](function pointer): function for extrat local descriptoers. (e.g. SIFT_extraction, DenseSIFT_extraction, etc)\n",
        "        ['num_codewords'](int):\n",
        "        ['result_dir'](str): Diretory to save codebooks & results.\n",
        "        \n",
        "    :param svm_params(dict): parameters for classifier training.\n",
        "        ['C'](float): Regularization parameter.\n",
        "        ['kernel'](str): Specifies the kernel type to be used in the algorithm.\n",
        "   \n",
        "    :return(sklearn.svm.SVC): trained classifier\n",
        "    \"\"\"\n",
        "    \n",
        "    extractor = feat_params['extractor']\n",
        "    k = feat_params['num_codewords']\n",
        "    result_dir = feat_params['result_dir']\n",
        "    \n",
        "    if not os.path.isdir(result_dir):\n",
        "        os.mkdir(result_dir)\n",
        "    \n",
        "    print(\"Load the training data...\")\n",
        "    start_time = time.time()\n",
        "    train_imgs, train_idxs = load_train_data(data_dir)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    print(\"Extract the local descriptors...\")\n",
        "    start_time = time.time()\n",
        "    train_des = extractor(train_imgs)\n",
        "    np.save(os.path.join(result_dir, 'train_des.npy'), train_des)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    del train_imgs\n",
        "    \n",
        "    print(\"Construct the bag of visual words...\")\n",
        "    start_time = time.time()\n",
        "    codebook = get_codebook(train_des, k)\n",
        "    np.save(os.path.join(result_dir, 'codebook.npy'), codebook)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "\n",
        "    print(\"Extract the image features...\")\n",
        "    start_time = time.time()    \n",
        "    train_features = extract_features(train_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'train_features.npy'), train_features)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "\n",
        "    del train_des, codebook\n",
        "    \n",
        "    print('Train the classifiers...')\n",
        "    accuracy = 0\n",
        "    models = {}\n",
        "    \n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_train.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(train_idxs, target_idxs)\n",
        "        \n",
        "        models[class_name] = train_classifier(train_features, target_labels, svm_params)\n",
        "        train_accuracy = models[class_name].score(train_features, target_labels) \n",
        "        print('{} Classifier train accuracy:  {:.4f}'.format(class_name ,train_accuracy))\n",
        "        accuracy += train_accuracy\n",
        "    \n",
        "    print('Average train accuracy: {:.4f}'.format(accuracy/len(category)))\n",
        "    del train_features, target_labels, target_idxs\n",
        "\n",
        "    return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkM12brUWjLs"
      },
      "source": [
        "feat_params = {'extractor': SIFT_extraction, 'num_codewords':1024, 'result_dir':os.path.join(data_dir,'sift_1024')}\n",
        "svm_params = {'C': 1, 'kernel': 'linear'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ULB-kc5jpk"
      },
      "source": [
        "- Below code will take about 30~70 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v_QngFiWlRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e5ba78a-2eb9-40a0-a7a8-5c45f5fdc6a8"
      },
      "source": [
        "models = Trainer(feat_params, svm_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load the training data...\n",
            "24.3603 seconds\n",
            "Extract the local descriptors...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "335.6140 seconds\n",
            "Construct the bag of visual words...\n",
            "3492.3808 seconds\n",
            "Extract the image features...\n",
            "49.1563 seconds\n",
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  1.0000\n",
            "car Classifier train accuracy:  1.0000\n",
            "horse Classifier train accuracy:  1.0000\n",
            "motorbike Classifier train accuracy:  1.0000\n",
            "person Classifier train accuracy:  0.9454\n",
            "Average train accuracy: 0.9891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLnPCHFOalSk"
      },
      "source": [
        "## Step 4: Test the classifier on validation set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EN0ZUiXWoI3"
      },
      "source": [
        "def Test(feat_params, models):\n",
        "    \"\"\"\n",
        "    Test the SVM classifier.\n",
        "\n",
        "    :param feat_params(dict): parameters for feature extraction.\n",
        "        ['extractor'](function pointer): function for extrat local descriptoers. (e.g. SIFT_extraction, DenseSIFT_extraction, etc)\n",
        "        ['num_codewords'](int):\n",
        "        ['result_dir'](str): Diretory to load codebooks & save results.\n",
        "        \n",
        "    :param models(dict): dict of classifiers(sklearn.svm.SVC)\n",
        "    \"\"\"\n",
        "    \n",
        "    extractor = feat_params['extractor']\n",
        "    k = feat_params['num_codewords']\n",
        "    result_dir = feat_params['result_dir']\n",
        "    \n",
        "    print(\"Load the validation data...\")\n",
        "    start_time = time.time()\n",
        "    val_imgs, val_idxs = load_val_data(data_dir)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    print(\"Extract the local descriptors...\")\n",
        "    start_time = time.time()\n",
        "    val_des = extractor(val_imgs)\n",
        "    np.save(os.path.join(result_dir, 'val_des.npy'), val_des)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    \n",
        "    del val_imgs\n",
        "    codebook = np.load(os.path.join(result_dir, 'codebook.npy'))\n",
        "    \n",
        "    print(\"Extract the image features...\")\n",
        "    start_time = time.time()    \n",
        "    val_features = extract_features(val_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'val_features.npy'), val_features)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "\n",
        "    del val_des, codebook\n",
        "\n",
        "    print('Test the classifiers...')\n",
        "    accuracy = 0\n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_val.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(val_idxs, target_idxs)\n",
        "        \n",
        "        val_accuracy = models[class_name].score(val_features, target_labels)\n",
        "        print('{} Classifier validation accuracy:  {:.4f}'.format(class_name ,val_accuracy))\n",
        "        accuracy += val_accuracy\n",
        "    \n",
        "    del val_features, target_idxs, target_labels\n",
        "\n",
        "    print('Average validation accuracy: {:.4f}'.format(accuracy/len(category)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z80KKyn7Ytfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "316325ac-41d9-4de1-b73a-1fa030ceb1d6"
      },
      "source": [
        "Test(feat_params ,models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load the validation data...\n",
            "26.1141 seconds\n",
            "Extract the local descriptors...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "312.0194 seconds\n",
            "Extract the image features...\n",
            "49.8063 seconds\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9406\n",
            "car Classifier validation accuracy:  0.7425\n",
            "horse Classifier validation accuracy:  0.9002\n",
            "motorbike Classifier validation accuracy:  0.9163\n",
            "person Classifier validation accuracy:  0.5829\n",
            "Average validation accuracy: 0.8165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3X0BFLm756K"
      },
      "source": [
        "## **Problem 5**: Implement Dense SIFT (10pt)\n",
        "Modify the feature extractor using the dense SIFT and evaluate the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaY4kqQE8PXK"
      },
      "source": [
        "def DenseSIFT_extraction(imgs):\n",
        "  \"\"\"\n",
        "  Extract Dense SIFT descriptors from images using cyvlfeat.sift.dsift().\n",
        "  Refer to https://github.com/menpo/cyvlfeat\n",
        "  You should set the parameters of cyvlfeat.sift.dsift() as bellow.\n",
        "    1.step = 12  2.float_descriptors = True\n",
        "\n",
        "  :param train_imgs(numpy.array): Gray-scale images in Numpy array format. shape:[num_images, width_size, height_size]\n",
        "  :return(numpy.array): Dense SIFT descriptors. shape:[num_images, num_des_of_each_img, 128]\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  collect_descriptor = []\n",
        "  for i in range(imgs.shape[0]):\n",
        "    f, d = cyvlfeat.sift.dsift(image=imgs[i], step=12, float_descriptors = True)\n",
        "    collect_descriptor.append(d) \n",
        "  return np.array(collect_descriptor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyhyW4yYEFxz"
      },
      "source": [
        "feat_params = {'extractor': DenseSIFT_extraction, 'num_codewords':1024, 'result_dir':os.path.join(data_dir,'dsift_1024')}\n",
        "svm_params = {'C': 1, 'kernel': 'linear'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPYn8ubgEuq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd28983a-0ff8-4fef-9c62-58e4a99ce627"
      },
      "source": [
        "models = Trainer(feat_params, svm_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load the training data...\n",
            "68.6236 seconds\n",
            "Extract the local descriptors...\n",
            "422.3650 seconds\n",
            "Construct the bag of visual words...\n",
            "7357.9805 seconds\n",
            "Extract the image features...\n",
            "100.3175 seconds\n",
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  1.0000\n",
            "car Classifier train accuracy:  1.0000\n",
            "horse Classifier train accuracy:  1.0000\n",
            "motorbike Classifier train accuracy:  1.0000\n",
            "person Classifier train accuracy:  0.9765\n",
            "Average train accuracy: 0.9953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b3X3gYHErl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9853c1f-d4ca-4838-f04b-0cf521ec17fc"
      },
      "source": [
        "Test(feat_params ,models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load the validation data...\n",
            "51.7456 seconds\n",
            "Extract the local descriptors...\n",
            "412.7966 seconds\n",
            "Extract the image features...\n",
            "95.2232 seconds\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9543\n",
            "car Classifier validation accuracy:  0.7963\n",
            "horse Classifier validation accuracy:  0.9163\n",
            "motorbike Classifier validation accuracy:  0.9131\n",
            "person Classifier validation accuracy:  0.5732\n",
            "Average validation accuracy: 0.8306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWyoU1yG7qhf"
      },
      "source": [
        "## **Problem 6**: Implement the Spatial Pyramid (10pt)\n",
        "Modify the feature extractor using the spatial pyramid matching and evaluate the performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJJpyKo98QQp"
      },
      "source": [
        "def SpatialPyramid(des, codebook):\n",
        "  \"\"\"\n",
        "  Extract image representation with Spatial Pyramid Matching using your DenseSIFT descripotrs & codebook.\n",
        "\n",
        "  :param des(numpy.array): DenseSIFT Descriptors.  shape:[num_images, num_des_of_each_img, 128]\n",
        "  :param codebook(numpy.array): Bag of visual words. shape:[k, 128]\n",
        "\n",
        "  :return(numpy.array): Image feature using SpatialPyramid [num_images, features_dim]\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  # level 0\n",
        "  k = codebook.shape[0]\n",
        "  num_des_each_image = des.shape[1] # 1600\n",
        "  num_des_each_edge = int(np.sqrt(num_des_each_image)) # 40\n",
        "  all_ = []\n",
        "  for index_img in range(des.shape[0]):\n",
        "    his_img = []\n",
        "    # level 0\n",
        "    bow0 = np.zeros(shape=k, dtype='float32')\n",
        "    img_all_des = des[index_img]\n",
        "    distance_matrix = euclidean_dist(img_all_des, codebook) # return (num_des_of_each_img, k)\n",
        "    min_indexs = np.argmin(distance_matrix,axis=1)\n",
        "    for j in range(len(min_indexs)):\n",
        "      bow0[min_indexs[j]] += 1\n",
        "    his_img.extend(bow0)\n",
        "    \n",
        "    # level 1\n",
        "    win_size1 = int(num_des_each_edge//2) # win_size1 =  20\n",
        "    bow1s = []\n",
        "    for j in range(4):\n",
        "      bow = np.zeros(shape=k, dtype='float32')\n",
        "      bow1s.append(bow)\n",
        "    for j in range(num_des_each_image):\n",
        "      closest_center_index = min_indexs[j]\n",
        "      x_cor = (j%num_des_each_edge)//win_size1\n",
        "      y_cor = (j//num_des_each_edge)//win_size1\n",
        "      index_bow = y_cor*2 + x_cor \n",
        "      bow1s[index_bow][closest_center_index] += 1 \n",
        "    for j in range(4):\n",
        "      his_img.extend(bow1s[j])\n",
        "\n",
        "    # level 2 \n",
        "    win_size2 = int(num_des_each_edge//4) # win_size2 =  10\n",
        "    bow2s = []\n",
        "    for j in range(16):\n",
        "      bow = np.zeros(shape=k, dtype='float32')\n",
        "      bow2s.append(bow)\n",
        "    for j in range(num_des_each_image):\n",
        "      closest_center_index = min_indexs[j]\n",
        "      x_cor = (j%num_des_each_edge)//win_size2\n",
        "      y_cor = (j//num_des_each_edge)//win_size2\n",
        "      index_bow = y_cor*4 + x_cor \n",
        "      bow2s[index_bow][closest_center_index] +=1\n",
        "    for j in range(16):\n",
        "      his_img.extend(bow2s[j])\n",
        "    all_.append(his_img)\n",
        "  return np.array(all_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAVKLXXyx2NE"
      },
      "source": [
        "def SP_Trainer(des_path, codebook_path, result_dir, svm_params):\n",
        "    \n",
        "    \"\"\"\n",
        "    Train the SVM classifier using SpatialPyramid representations.\n",
        "\n",
        "    :param des_path(str): path for loading training dataset DenseSIFT descriptors.\n",
        "    :param codebook(str): path for loading codebook for DenseSIFT descriptors.\n",
        "    :param result_dir(str): diretory to save features.\n",
        "        \n",
        "    :param svm_params(dict): parameters for classifier training.\n",
        "        ['C'](float): Regularization parameter.\n",
        "        ['kernel'](str): Specifies the kernel type to be used in the algorithm.\n",
        "   \n",
        "    :return(sklearn.svm.SVC): trained classifier\n",
        "    \"\"\"\n",
        "    train_idxs = load_train_idxs(data_dir)\n",
        "    train_des = np.load(des_path)\n",
        "    codebook = np.load(codebook_path)\n",
        "    train_features = SpatialPyramid(train_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'train_sp_features.npy'), train_features)\n",
        "\n",
        "    del train_des, codebook\n",
        "    \n",
        "    print('Train the classifiers...')\n",
        "    accuracy = 0\n",
        "    models = {}\n",
        "    \n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_train.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(train_idxs, target_idxs)\n",
        "        \n",
        "        models[class_name] = train_classifier(train_features, target_labels, svm_params)\n",
        "        train_accuracy = models[class_name].score(train_features, target_labels) \n",
        "        print('{} Classifier train accuracy:  {:.4f}'.format(class_name ,train_accuracy))\n",
        "        accuracy += train_accuracy\n",
        "    \n",
        "    print('Average train accuracy: {:.4f}'.format(accuracy/len(category)))\n",
        "    del train_features, target_labels, target_idxs\n",
        "\n",
        "    return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q--UT0fyEyc"
      },
      "source": [
        "def SP_Test(des_path, codebook_path, result_dir, models):\n",
        "    \"\"\"\n",
        "    Test the SVM classifier.\n",
        "\n",
        "    :param des_path(str): path for loading validation dataset DenseSIFT descriptors.\n",
        "    :param codebook(str): path for loading codebook for DenseSIFT descriptors.\n",
        "    :param result_dir(str): diretory to save features.      \n",
        "    :param models(dict): dict of classifiers(sklearn.svm.SVC)\n",
        "\n",
        "    \"\"\" \n",
        "    val_idxs = load_val_idxs(data_dir)\n",
        "    val_des = np.load(des_path)\n",
        "    codebook = np.load(codebook_path)\n",
        "    val_features = SpatialPyramid(val_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'val_sp_features.npy'), val_features)\n",
        "\n",
        "\n",
        "    del val_des, codebook\n",
        "\n",
        "    print('Test the classifiers...')\n",
        "    accuracy = 0\n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_val.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(val_idxs, target_idxs)\n",
        "        \n",
        "        val_accuracy = models[class_name].score(val_features, target_labels)\n",
        "        print('{} Classifier validation accuracy:  {:.4f}'.format(class_name ,val_accuracy))\n",
        "        accuracy += val_accuracy\n",
        "\n",
        "    del val_features, target_idxs, target_labels\n",
        "\n",
        "    print('Average validation accuracy: {:.4f}'.format(accuracy/len(category)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS7Svvy2zTv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a95d93b-a516-4c11-ae66-118f56f8ae87"
      },
      "source": [
        "#YOUR CODE HERE for training & testing with Spatial Pyramid\n",
        "dense_descriptors_result_dir = os.path.join(data_dir,'dsift_1024')\n",
        "train_dense_descriptors_path = os.path.join(dense_descriptors_result_dir, 'train_des.npy')\n",
        "codebook_dense_descriptors_path = os.path.join(dense_descriptors_result_dir,'codebook.npy')\n",
        "\n",
        "sp_result_dir = os.path.join(data_dir, 'sp')\n",
        "if not os.path.isdir(sp_result_dir):\n",
        "    os.mkdir(sp_result_dir)\n",
        "\n",
        "sp_svm_params = {'C': 1, 'kernel': 'linear'}\n",
        "sp_models = SP_Trainer(train_dense_descriptors_path, codebook_dense_descriptors_path , sp_result_dir, sp_svm_params)\n",
        "val_dense_descriptors_path = os.path.join(dense_descriptors_result_dir,'val_des.npy')\n",
        "SP_Test(val_dense_descriptors_path, codebook_dense_descriptors_path, sp_result_dir, sp_models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  1.0000\n",
            "car Classifier train accuracy:  1.0000\n",
            "horse Classifier train accuracy:  1.0000\n",
            "motorbike Classifier train accuracy:  1.0000\n",
            "person Classifier train accuracy:  1.0000\n",
            "Average train accuracy: 1.0000\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9620\n",
            "car Classifier validation accuracy:  0.8581\n",
            "horse Classifier validation accuracy:  0.9507\n",
            "motorbike Classifier validation accuracy:  0.9466\n",
            "person Classifier validation accuracy:  0.6649\n",
            "Average validation accuracy: 0.8765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "401jsdB_8CA1"
      },
      "source": [
        "## **Problem 7**: Improve classification using non-linear SVM (10pt)\n",
        "Modify the classifier using the non-linear SVM and evaluate the performance. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg162rmJ8Q8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f3d2e4-377e-44d1-f9c9-67db7cb275b2"
      },
      "source": [
        "# YOUR CODE HERE to improve classification using non-linear SVM\n",
        "# YOUR CODE should include training & testing with non-linear SVM.\n",
        "\n",
        "# use rbf kernel to achieve non-linear SVM \n",
        "rbf_feat_params= {'extractor': SIFT_extraction, 'num_codewords':1024, 'result_dir':os.path.join(data_dir,'non_linear')}\n",
        "rbf_svm_params = {'C': 1, 'kernel': 'rbf'}\n",
        "rbf_models = Trainer(rbf_feat_params, rbf_svm_params)\n",
        "Test(rbf_feat_params, rbf_models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load the training data...\n",
            "79.0991 seconds\n",
            "Extract the local descriptors...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "341.9228 seconds\n",
            "Construct the bag of visual words...\n",
            "3499.6530 seconds\n",
            "Extract the image features...\n",
            "45.8527 seconds\n",
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  0.9664\n",
            "car Classifier train accuracy:  0.8641\n",
            "horse Classifier train accuracy:  0.9539\n",
            "motorbike Classifier train accuracy:  0.9531\n",
            "person Classifier train accuracy:  0.9066\n",
            "Average train accuracy: 0.9288\n",
            "Load the validation data...\n",
            "59.8539 seconds\n",
            "Extract the local descriptors...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "305.2727 seconds\n",
            "Extract the image features...\n",
            "47.2216 seconds\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9491\n",
            "car Classifier validation accuracy:  0.8638\n",
            "horse Classifier validation accuracy:  0.9402\n",
            "motorbike Classifier validation accuracy:  0.9495\n",
            "person Classifier validation accuracy:  0.6576\n",
            "Average validation accuracy: 0.8720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b8Z-UnocePF"
      },
      "source": [
        "# <font color=\"blue\"> Discussion and Analysis </font>\n",
        "## Discussion Guidelines\n",
        "- You should write discussion about **Problem 5 ~ Problem 7**.\n",
        "- Simply reporting the results (e.g. classification accuracy) is not considered as a discussion.\n",
        "- For each problem's discussion, you should explain and compare how each method improves the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCn1KAvNc38r"
      },
      "source": [
        "Please write discussions on the results above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMgoK9aPWv4a"
      },
      "source": [
        "Discussion for problem 5:\n",
        "Compare training accuracy and testing accuracy of the model that uses SIFT_extraction(from problem 4) vs the model that uses DenseSift_extraction(from problem 5):\n",
        "Training accuracy:\n",
        "                      Model uses SIFT_extraction            Model uses Densesift_extraction\n",
        "Aeroplane                   1.0                        |                 1.0\n",
        "Car                         1.0                        |                 1.0\n",
        "Horse                       1.0                        |                 1.0\n",
        "Motorbike                   1.0                        |                 1.0\n",
        "Person                      0.9454                     |                 0.9765\n",
        "Average accuracy            0.9891                     |                 0.9953\n",
        "\n",
        "\n",
        "Testing accuracy: \n",
        "                      Model uses SIFT_extraction            Model uses Densesift_extraction\n",
        "Aeroplane                   0.9406                     |                 0.9543\n",
        "Car                         0.7425                     |                 0.7963\n",
        "Horse                       0.9002                     |                 0.9163\n",
        "Motorbike                   0.9163                     |                 0.9131\n",
        "Person                      0.5829                     |                 0.5732\n",
        "Average accuracy            0.8165                     |                 0.8306\n",
        "\n",
        "Discuss: From the above statistics, we can observe that applying densesift descriptor instead of sift descriptor brings a good improvement in both training accuracy \n",
        "and testing accuracy. For training accuracy, we can see an increment of about 3% in class 'person' and an increment of about 1% in the average accuracy of 5 classes.\n",
        "For testing accuracy, the improvement can be seen in all 5 classes, and class 'Car' observes the largest improvement, which is about 5%. Overall, the average testing accuracy\n",
        "is improved by about 1.5%. This result makes sense because in general, densesift descriptor performs better than sift descriptor for some reasons. Firstly, densesift\n",
        "descriptor provides a dense set of feature that encode spatial information of the image (which is lack in sift descriptor) and furthermore, it also helps to overcome\n",
        "the problem of blob detection sensitivity in sift descriptor. Therefore, i believe that my implementation for problem 4 and 5 works quite well and provides a \n",
        "reasonable result.  \n",
        "\n",
        "\n",
        "Discussion for problem 6:\n",
        "Compare training accuracy and testing accuracy of the normal implementation (from problem 4) vs the implementation that uses spatial pyramid(from problem 6):\n",
        "Training accuracy: \n",
        "                       Normal implementation                   Implementation uses spatial pyramid\n",
        "Aeroplane                   1.0                        |                1.0\n",
        "Car                         1.0                        |                1.0\n",
        "Horse                       1.0                        |                1.0\n",
        "Motorbike                   1.0                        |                1.0\n",
        "Person                      0.9454                     |                1.0\n",
        "Average accuracy            0.9891                     |                1.0\n",
        "\n",
        "Testing accuracy: \n",
        "                       Normal implementation                   Implementation uses spatial pyramid\n",
        "Aeroplane                   0.9406                     |                0.9620\n",
        "Car                         0.7425                     |                0.8581\n",
        "Horse                       0.9002                     |                0.9507\n",
        "Motorbike                   0.9163                     |                0.9466\n",
        "Person                      0.5829                     |                0.6649\n",
        "Average accuracy            0.8165                     |                0.8765\n",
        "\n",
        "Discuss: From the above statistics, we can observe that applying spatial pyramid brings a very impressive improvement for both training and testing accuracy. \n",
        "For training accuracy, applying spatial pyramid helps boost accuracies of all 5 classes to maximum (1.0 is maximum accuracy), which is a very desirable result. \n",
        "The improvement can be seen in class 'person', which is about 5.5% and overall, the average training accuracy was increased by about 1%. For testing accuracy, \n",
        "the improvement can be seen in all 5 classes, and it's worth noticing that the improvement is by at least 2% for all classes. The biggest improvement is observed \n",
        "in class 'car', which is an increment of 11%. Overall, the testing accuracy is improved by about 6%. I believe that these improvements are very reasonable because\n",
        "in fact spatial pyraid matching brings a lot of benefits to the model: Firstly, it provides a histogram that encodes a notion of spatial information, which is \n",
        "useful for the classification task. Secondly, this method combines multiple resolutions in a principled fashion, therefore it's robust to failures at individual\n",
        "levels. Furthermore, in our implementation for spatial pyramid matching (problem 6), we extract features from images by using DenseSift descriptor, which helps \n",
        "to overcome the problem of blob detection sensitivity in Sift descriptor. Because of these benefits, the spatial pyramid matching technique brings a big improvement \n",
        "to the performance of our model. \n",
        "\n",
        "\n",
        "Discussion for problem 7: \n",
        "Compare training accuracy and testing accuracy of the linear SVM (from problem 4) vs the non-linear SVM (problem 7):\n",
        "Training accuracy: \n",
        "                        Linear SVM                                  Non-linear SVM \n",
        "Aeroplane                  1.0                         |                0.9664\n",
        "Car                        1.0                         |                0.8641\n",
        "Horse                      1.0                         |                0.9539\n",
        "Motorbike                  1.0                         |                0.9531\n",
        "Person                     0.9454                      |                0.9066\n",
        "Average accuracy           0.9891                      |                0.9288\n",
        "\n",
        "Testing accuracy: \n",
        "                        Linear SVM                                  Non-linear SVM\n",
        "Aeroplane                  0.9406                      |                0.9491\n",
        "Car                        0.7425                      |                0.8638\n",
        "Horse                      0.9002                      |                0.9402\n",
        "Motorbike                  0.9163                      |                0.9495\n",
        "Person                     0.5829                      |                0.6576\n",
        "Average accuracy           0.8165                      |                0.8720\n",
        "\n",
        "Discuss: From the above statistics, we can observe that the non-linear SVM has a worse training accuracy than the linear SVM for all classes. In my opinion, this happens\n",
        "because in my implementation for non-linear SVM, i use 'rbf' kernel for the SVM but unfortunately this kernel is strongly affected by the magnitude of the vector \n",
        "(indeed, the formula of 'rbf' kernel is exp(-y||xi-xj||^2), which is strongly affected by magnitude of xi,xj). And furthermore, in my implementation, i don't normalize\n",
        "the histogram before fetching into SVM, therefore the magnitude of the input feature for SVM would be very large and it strongly affects the performance of 'rbf' kernel.\n",
        "I think that to overcome this issue, we can normalize the historam before fetching into the non-linear SVM or using another hyperparameter setting for the non-linear SVM \n",
        "because the current hyperparameter setting may be not optimal for our non-linear SVM. For testing accuracy, we can see that applying non-linear kernel brings a very good \n",
        "improvement to the accuracy in all classes. The biggest improvement could be seen in class 'car' (which is 12%) and class 'person' (which is 7.5%). Overall, the non-linear \n",
        "kernel boosts the average testing accuracy by 6%, which is quite impressive. In my opinion, this testing accuracy improvement happens due to the fact that our image is \n",
        "complex and not linearly separable, therefore a simple linear SVM is not sufficient for SVM to separate images of different categories. But with a non-linear kernel, then \n",
        "SVM can map data from original input space into some higher dimensional space where separation of data is much easier. It is the reason why in my implementation, the \n",
        "non-linear SVM gains a higher testing accuracy than linear SVM. \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}